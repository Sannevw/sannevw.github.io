<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Sanne van Waveren</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.1/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Sanne van Waveren</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profilepicture.jpg" alt="Profile Picture of Sanne van Waveren" /></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#cv">CV</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#thesisproposals">Master Thesis Proposals</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#research">Research</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#contact">Contact</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                  <img class="img-fluid img-logo2 mx-auto d-block" src="assets/img/logo.png" alt="Logo with a graphical robot icon and the text 'Sanne van Waveren' below it." />
                  <div class="row">
                    <div class="col"></div>
                    <div class="col-8">
                  <h2 class="text-center">Gathering Collective Intelligence For Robots</h2>
                </div>
                  <div class="col"></div>
                </div>
                  <div class="row">
                    <div class="col"></div>
                    <div class="col-8">
                  <p class="text-center">Human-Robot Interaction, Social Robotics, Computer Science, Crowdsourcing, AI </p>
                </div>
                <div class="col"></div>
                </div>
                    <!-- <h2 class="mb-0">
                        Sanne van Waveren
                    </h2> -->
                    <div class="text-center subheading mb-5">
                        <a href="https://scholar.google.com/citations?user=B80fZLUAAAAJ&hl=en">Link to Google Scholar --</a>
                        <a href="mailto:sannevw@kth.se">sannevw@kth.se</a>
                    </div>
                    <div class="row">
                    <div class="col"></div>
                    <div class="col-8">
                    <p class="text-justify lead">I am a PhD student at <a href="https://kth.se/is/rpl"> Robotics, Perception, and Learning (RPL)</a> at the Royal Institute of Technology (KTH), under the supervision of <a href="https://iolandaleite.com">dr. Iolanda Leite.</a> Previously, I obtained a computer science Master degree from the University of Twente in the Netherlands. My interests lie in the use of technology to enhance people’s lives. By combining social sciences and computer science, the goal of my PhD research is to develop social robots that can interact with people in an appropriate, effective, and efficient way over longer periods of time. </p>
                  </div>
                    <div class="col"></div>

                  </div>

                    <div class="d-flex justify-content-around">
                  <p class="bg-light text-black-50 text-justify w-50 ">
                  Looking to do your master thesis on Human-Robot Interaction? See my <a href="#thesisproposals"> master thesis proposals</a>, or feel free to contact me and tell me about your own idea.
                  </p>
                </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Experience-->
            <section class="resume-section" id="cv">
                <div class="resume-section-content">
                  <div class="row">
                    <div class="col"></div>
                    <h2 class="col-md-4">CV / Academic Service </h2>
                  <div class="col"></div>
                  </div>
                  <div class="row">
                    <div class="col"></div>
                    <div class="col-8">
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3>Reviewer for Conference Proceedings</h3>
                           <ul>
                             <li> ACM/IEEE International Conference on Human-Robot Interaction (HRI), 2019, 2020, <p id="awardpar"> 2021 Special recognition for outstanding review </p> </li>
                             <li> ACM CHI Conference on Human Factors in Computing Systems (CHI), <p id="awardpar"> 2020 Special recognition for outstanding review </p> </li>
                             <li> CogSci 2020, The 42nd Annual Meeting of the Cognitive Science Society </li>
                             <li> Robotics: Science and Systems, 2020 </li>
                             <li> ACM International Conference on Multimodal Interaction (ICMI), 2019, 2020 </li>
                             <li> International Conference on Affective Computing & Intelligent Interaction (ACII), 2019 </li>
                             <li> International Conference on Human-Agent Interaction (HAI), 2019 </li>
                             <li> ACM Interaction Design and Children (IDC) conference, 2019, 2020 </li>
                          </ul>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">2019 - Present</span></div>
                      </div>
                    </div>
                    <div class="col"></div>
                    </div>
                    <div class="row">
                    <div class="col"></div>
                    <div class="col-8">
                      <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Reviewer for Journals</h3>
                           <ul>
                             <li> Springer International Journal of Social Robotics, Impact Factor: 2.296 (2018) </li>
                             <li> IEEE Transactions on Affective Computing, Impact Factor: 6.288 (2020) </li>
                             <li> Frontiers in Robotics and AI, Impact Factor: 3.310 (2018) </li>
                             <li> Frontiers in Psychology, Impact Factor: 2.067 (2020) </li>
                          </ul>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">2019 - Present</span></div>
                    </div>
                  </div>
                  <div class="col"></div>
                  </div>

                  <div class="row">
                  <div class="col"></div>
                  <div class="col-8">
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                      <div class="flex-grow-1">
                        <h3 class="mb-0">Student Volunteer </h3>
                        <ul>
                        <li>
                          International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2018
                        </li>
                      </ul>
                      </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                      <div class="flex-grow-1">
                          <h3 class="mb-0">Student Supervision</h3>
                          <h4> Master Theses</h4>
                         <ul>
                           <li> Fredrik Sebek, Title: TBD, Feb 2021 – current </li>
                           <li> Rui Li, <i>Human Robot Interaction in Virtual Reality: Comparing human-robot proxemic behavior in reality versus virtual reality</i>, 2018 </li>
                        </ul>
                        <h4> Course Projects</h4>
                       <ul>
                         <li> DT2140 Multimodal Interaction and Interfaces —  Irene Kaklopoulou, Thays Santos Duarte, Ziyi Zhu, Yuqi Liu, Nov 2020-Jan 2021 </li>
                         <li> DD2411 Research project in Robotics, Perception and Learning — Oscar Örnberg, Feb-Sept 2019 </li>
                      </ul>
                      </div>
                      <div class="flex-shrink-0"><span class="text-primary">2019 - Present</span></div>
                  </div>

                  <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                      <h3 class="mb-0">Awards and Scholarships </h3>
                      <ul>
                      <li>
                        SIGAI ACM travel grant, 2019
                      </li>
                    </ul>
                    </div>
                  </div>

                  <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                    <div class="flex-grow-1">
                      <h3 class="mb-0">Media Outreach / Public Events </h3>
                      <ul>
                      <li>
                        KTH Giants, November 2019
                      </li>
                    </ul>
                    </div>
                  </div>

                  <div class="embed-responsive embed-responsive-16by9 w-50 mx-auto d-block">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/Vqg08iNuXDo" alt="An introduction video to our work in the Robotics, Perception, and Learning Division." frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
              </div>

              <div class="col"></div>
            </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Education-->
            <section class="resume-section" id="publications">
                <div class="resume-section-content">
                  <div class="row">
                    <div class="col"></div>
                    <h2 class="col-md-4">Publications </h2>
                  <div class="col"></div>
                  </div>
                  <div class="row">
                    <div class="col"></div>
                    <div class="col-8">
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3>Conference Papers</h3>
                            <div class="subheading mb-3">Full-Length, Peer-reviewed</div>

                            <div class="text-justify">
                            <ul>
                            <li class="mt-2"> <div class="award"> Taras Kucherenko, Patrik Jonell, <b>Sanne van Waveren</b>, Gustav Eje Henter, Simon Alexanderson, Iolanda Leite, and Hedvig Kjellström. Gesticulator: A framework for semantically-aware speech-driven gesture generation. International Conference on Multimodal Interaction (ICMI ‘20). 2020. <img class="img-fluid img-profile rounded-circle mx-auto w-3 h-3" src="assets/img/award.png" alt="Medal Icon" />  <p id="awardpar"> Best Paper Award. </p> <a href="https://dl.acm.org/doi/10.1145/3382507.3418815"> [Link to paper] </a> <a href="https://github.com/svito-zar/Gesticulator"> [Link to code] </a> <a href="https://svito-zar.github.io/gesticulator/">[Link to project page]</a>. </div> </li>

                            <li class="mt-2">
                              Kontogiorgos, D., <b>van Waveren, S.</b>, Wallberg, O., Abelho Pereira, A. T., Leite, I., & Gustafson, J. (2020). Embodiment Effects in Interactions with Failing Robots. In <i>SIGCHI Conference on Human Factors in Computing Systems (CHI)</i>. <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376372?casa_token=iXfBHqarN9wAAAAA:HPLjOdXA_35vgvhd69amJCto7HGMTJ7ouN8hBTVGXvG3z3ZJ53o9dRhDB34ahWyqiFn0KcSHaV8" > [Link to paper] </a> <a href="https://www.youtube.com/watch?v=MxGerI0Bc-U&feature=youtu.be"> [Link to video] </a> [<i>Acceptance rate 24.3%</i>]
                            </li>

                            <li class="mt-2">
                              Kontogiorgos, D., Pereira, A., Sahindal, B., <b>van Waveren, S.</b>, & Gustafson, J. (2020, March). Behavioural Responses to Robot Conversational Failures. In <i>Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction</i> (pp. 53-62). <a href="https://dl.acm.org/doi/abs/10.1145/3319502.3374782" > [Link to paper] </a>
                            </li>
                            </ul>
                          </div>

                            <!-- <p>GPA: 3.23</p> -->
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">2020</span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0"></h3>
                            <div class="subheading mb-3"></div>

                            <div class="text-justify">
                            <ul>
                            <li class="mt-2">
                            Li, R., van Almkerk, M., <b>van Waveren, S.</b>, Carter, E., & Leite, I. (2019, March). Comparing Human-Robot Proxemics between Virtual Reality and the Real World. In <i>2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</i> (pp. 431-439). IEEE. <a href="https://ieeexplore.ieee.org/abstract/document/8673116"> [Link to paper] </a> [<i>Acceptance rate 24%</i>]
                            </li>

                            <li class="mt-2">
                              <b>van Waveren, S.</b>, Carter, E. J., & Leite, I. (2019, July). Take One For the Team: The Effects of Error Severity in Collaborative Tasks with Social Robots. In <i>Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents</i> (pp. 151-158). ACM. <a href="https://dl.acm.org/doi/10.1145/3308532.3329475" > [Link to paper] </a> [<i>Acceptance rate 24%</i>]
                            </li>

                            <li class="mt-2">
                              <b>van Waveren S.</b>, Björklund, L., Carter, E. J., & Leite, I. (2019, November). Knock on wood: The effects of material choice on the perception of social robots. In <i> International Conference on Social Robotics (pp. 211-221). Springer, Cham. </i> <a href="https://link.springer.com/chapter/10.1007/978-3-030-35888-4_20"> [Link to paper] </a> [<i>Acceptance rate 75% </i>]
                            </li>
                            </ul>
                          </div>

                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">2019</span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Workshop Papers</h3>
                            <div class="subheading mb-3">Peer-reviewed</div>

                            <div class="text-justify">
                            <ul>
                            <li class="mt-2">
                            Joosse, M. P., <b>van Waveren, S.</b>, Zaga, C., & Evers, V. (2017) Groups in Conflict at the Airport: How Everyday People Think a Robot Should Act. Poster presented at the CSCW’17 Workshop on Robots in Groups and Teams. <a href="https://cpb-us-w2.wpmucdn.com/sites.coecis.cornell.edu/dist/c/16/files/2017/01/Joosse-van-Waveren-Zaga-and-Evers-29wd7w7.pdf" > Link to Paper </a>
                            </li>

                            </ul>
                          </div>

                </div>
              </div>
            </div>
            <div class="col"></div>


            </section>
            <hr class="m-0" />
            <!-- Skills-->
            <section class="resume-section" id="thesisproposals">
                <div class="resume-section-content">
                  <div class="row">
                    <div class="col"></div>
                    <h2 class="col-md-8">Master Thesis Proposals </h2>
                  <div class="col"></div>
                  </div>
                    <div class="row">
                      <div class="col"></div>
                      <div class="col-8">
                    <p class="text-justify">
                    I am continuously looking for highly motivated, independent masters students who are interested in (i) developing new algorithms and crowdsourcing approaches and (ii) implementing this in simulation or on a real robot, with the aim of efficiently authoring and verifying new robot behaviors for human-robot interaction. If you are interested, feel free to contact me.
                    </p>
                    <p>
                      I will be your daily supervisor and Associate Professor Dr. <a href="https://iolandaleite.com/">Iolanda Leite</a> will be your examiner.
                    </p>
                    <h3 class="mb-5">Proposals </h3>
                    <h4 class="mb-5">PROP 1. Human-Centered Approach for Semi-situated Robot Learning through Crowdsourcing </h4>
                    <p class="text-justify"> As robots move out of controlled industrial environments into the real world, a persistent challenge is the need to expand behavioral policies on a large scale without constant expert supervision. To this end, this thesis explores opportunities for crowdsourcing,  a method that has been key to some recent breakthroughs in computer vision, to gather new robot behavior. One challenge is how, with partial information about the robot environment, people can provide the robot with useful input when it gets stuck to speed up the learning process. Such semi-situated robot learning has shown promising for gathering verbal and non-verbal dialog behaviors for repeated interactions with humans [1]. In this thesis, the goal is to:
                    </p>
                    <ul>
                      <li>
                      Familiarize with the existing literature on non-expert robot programming and crowdsourcing HRI; starting literature will be provided, but it is expected that a full literature review is done as a basis for the thesis;
                      </li>
                      <li>
                        Identify an opportunity for robot teaching through crowdsourcing;
                      </li>
                      <li>
                        Implement a running example of the crowdsourcing pipeline.
                      </li>
                    </ul>

                    <p>Prerequisites:</p>
                    <ul><li>
                    Good programming knowledge (Javascript, Python)
                    </li> </ul>
                    <p class="text-justify">
                      [1] Leite, I., Pereira, A., Funkhouser, A., Li, B., & Lehman, J. F. (2016, October). Semi-situated learning of verbal and nonverbal content for repeated human-robot interaction. In <i>Proceedings of the 18th ACM International Conference on Multimodal Interaction </i> (pp. 13-20).
                    </p>

                    <h4 class="mb-5  mt-5">PROP 2. Perceived Safety in Human-Drone Interaction</h4>

                    <p class="text-justify">
                      Safety is crucial when it comes to the deployment of robots in the real world. A special case is when a robot operates in an environment that is shared with people, people who can be seen as agents with their own intentions, beliefs, and actions that the robot needs to anticipate. This raises new questions related to the verification of robot behaviors [1].</p>
                      <p class="text-justify">
                      Human-Drone Interaction is increasingly receiving attention as drones are entering spaces that they will share with people. Drones can be for example used for surveillance of a construction site, to make an inventory of materials and report progress. In such scenarios, it is crucial to fly around and interact with humans in a comfortable and socially acceptable way. The goal of this thesis is to:

                    </p>

                  <ul>
                    <li>
                      Familiarize with the existing literature on human-drone interaction in relation to perceived safety, starting literature will be provided, but it is expected that a full literature review is done as a basis for the thesis;
                    </li>
                      <li>
                    Identify an opportunity to enhance perceived safety in HDI;
                  </li>
                    <li>
                    Design and implement a user study to evaluate the proposed opportunity with real people.
                  </li>
                  </ul>
                    Prerequisites:
                    <ul><li>
                    Good programming knowledge (Python and/or C++)</li>
                    <li>
                      Interest in human-robot interaction studies
                    </li>
                  </ul>
                    <p class="text-justify">
                    [1]  Alami, R., Eder, K. I., Hoffman, G., & Kress-Gazit, H. (2019). Verification and Synthesis of Human-Robot Interaction (Dagstuhl Seminar 19081). In Dagstuhl Reports (Vol. 9, No. 2). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.
                  </p>

                  <h4 class="mb-5 mt-5">PROP 3. Synthesis-Aided Non-experts Robot Programming</h4>
                  <p class="text-justify"> To date, robot programming has primarily been a task for engineers, requiring a high level of mathematical and programming knowledge. Recently, an increasing body of work has focused on approaches to quickly and intuitively allow non-experts to program robots [e.g., 1,2,3,4]. One challenge is what parts of the programming process can be automated; specifically, how we can capture human input and translate it into specifications used for synthesis [5]. The goal of this master thesis is to: </p>

                  <ul><li>
                    Familiarize with the existing literature on non-expert robot programming and program synthesis, starting literature will be provided, but it is expected that a full literature review is done as a basis for the thesis;</li>
                    <li>
                    Identify an opportunity for applying synthesis to improve the process of synthesis-aided robot programming;</li>
                    <li>
                    Implement a running example and ideally, evaluate its effectiveness. </li>
                  </li></ul>

                <p>  Prerequisites: </p>

                <ul><li>
                  Good programming knowledge;
                </li>
                  <li>Knowledge of formal methods: program synthesis and verification.</li>
              </ul>
              <p class="text-justify">
                [1] Orendt, E. M., Fichtner, M., & Henrich, D. (2016, August). Robot programming by non-experts: intuitiveness and robustness of one-shot robot programming. In <i>2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN) </i> (pp. 192-199). IEEE.
              </p>
              <p class="text-justify">
                [2] Liang, Y. S., Pellier, D., Fiorino, H., & Pesty, S. (2017). Evaluation of a robot programming framework for non-experts using symbolic planning representations. In <i>2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN) </i> (pp. 1121-1126). IEEE.
              </p>

              <p class="text-justify">
                3] Sefidgar, Y. S., & Cakmak, M. (2018, March). End-user programming of manipulator robots in situated tangible programming paradigm. In <i>Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction </i> (pp. 319-320).
              </p>

              <p class="text-justify">
                [4] Stenmark, M., Haage, M., & Topp, E. A. (2017, March). Simplified programming of re-usable skills on a safe industrial robot: Prototype and evaluation. In <i> Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction  </i> (pp. 463-472). </p>

              <p class="text-justify"> [5]  Alami, R., Eder, K. I., Hoffman, G., & Kress-Gazit, H. (2019). Verification and Synthesis of Human-Robot Interaction (Dagstuhl Seminar 19081). In <i>Dagstuhl Reports</i> (Vol. 9, No. 2). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik. </p>

              <h4 class="mb-5  mt-5">PROP 4. Natural Person-Following Behavior for Robots</h4>
              <p class="text-justify">
                Mobile robots are increasingly deployed in spaces that they will share with humans and, in addition to purely technical safety guarantees, it is crucial to endow these robots with socially acceptable navigation skills. For example, the robot needs to behave such that it is perceived as socially normative and does not cause discomfort in the people it shares the space with. For example, the term proxemics (i.e., the social use of space) was coined by Hall [1] and defines four personal space zones ranging from intimate (0-0.45 meter) to public (> 3.6 meter), which can be critical for successful HRI [2].
              </p>

              <p class="text-justify">
                Prior work has explored different person-following behaviors and studied the effect of various aspects, such as different person-tracking algorithms, relative angle, velocity, acceleration. Gockley at al. [3] studied how human-likeness, personal space, reliability, and safety affected a robot’s person-following behaviors: one following the exact path of the person and the other following in the direction of the person. Results showed that direction-following behavior appeared more natural and human-like and more closely matched people’s expectations. In this thesis, the goal is to:
              </p>

              <ul>
                <li> Familiarize with the existing literature on proxemics in person-following HRI scenarios, starting literature will be provided, but it is expected that a full literature review is done as a basis for the thesis; </li>
                <li>Identify factors in the robot’s following behavior that may influence people’s perceived comfort, perceived safety, or other subjective measures; </li>
                <li> Design and implement a user study to evaluate how these factors influence how people perceive the robot in a person-following interaction. </li>
              </ul>

              <p>Prerequisites:</p>
              <ul>
                <li> Good programming knowledge (Python); </li>
                <li>Interest in human-robot interaction studies; </li>
                <li>Experience with robots is a plus. </li>
              </ul>

              <p class="text-justify">
                [1]  Hall, E. T. (1966). <i>The hidden dimension</i> (Vol. 609). Garden City, NY: Doubleday.
              </p>
              <p class="text-justify">
                [2] Mead, R., & Matarić, M. J. (2016). Perceptual models of human-robot proxemics. In <i>Experimental Robotics</i> (pp. 261-276). Springer, Cham.</p>
                <p class="text-justify">
                  [3] Gockley, R., Forlizzi, J., & Simmons, R. (2007, March). Natural person-following behavior for social robots. In <i>Proceedings of the ACM/IEEE international conference on Human-robot interaction </i> (pp. 17-24). </p>
                  <p class="text-justify">
                    [4] Scales, P., Aycard, O., & Auberge, V. (2020). Studying Navigation as a Form of Interaction: a Design Approach for Social Robot Navigation Methods. In <i>IEEE International Conference on Robotics and Automation (ICRA). </i> </p>

                </div>

              <div class="col"></div>
            </div>
            </section>
            <hr class="m-0" />

            <!-- Interests-->
            <section class="resume-section" id="research">
                <div class="resume-section-content">
                  <div class="row">
                    <div class="col"></div>
                    <h2 class="col-8">Research </h2>
                  <div class="col"></div>
                  </div>

                  <div class="row">
                    <div class="col"></div>
                    <div class="col-8">

                    <h3 class="mb-5"> Current -- Gather Collective Intelligence for Robots </h3>
                    <p class="text-justify"> My current work revolves around enabling robots to gather additional behaviors during deployment. It is impossible to anticipate all possible cases at which a robot/interaction may breakdown. Endowing robots with the ability to collect additional input on the fly is therefore crucial for them to succeed in the real world. I am interested in how we can exploit our ‘collective intelligence’ (common knowledge) to let non-experts (people without programming or robotics experience) provide input to the robot.
                    </p>
                    <p class="text-justify"> In the real world, robots will encounter many new situations to which they need to adapt and respond. We, humans, are rather successful at adapting in new situations, and have substantial knowledge on what is socially acceptable and what is not. Currently, we are exploring the use of collective intelligence – the intelligence that we, humans, collectively have – for social robots. If a robot fails, because it does not know what to do or how to act, can someone without (robotic) programming skills help the robot to resolve the failure? For example, by providing the robot with the correct next action. </p>

                    <div class="embed-responsive embed-responsive-16by9 w-50 mx-auto d-block">
                    <iframe src="https://www.youtube.com/embed/M_wdpGgWFk0" alt="An introduction video to our work in the Social Robotics Lab" align="center" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  </div>

                  <h3 class="mb-5 pt-lg-5"> Previous Projects </h3>
                  <h4 class="mb-5  mt-5">Human-Agent Negotation in Virtual Reality </h4>

                  <p class="text-justify">Negotiation with Virtual Humans, Visiting Research @ <a href="https://ict.usc.edu/"> USC Institute for Creative Technologies (ICT) </a>, under the supervision of <a href="https://people.ict.usc.edu/~gratch/"> dr. Jonathan Gratch</a>, Director for Virtual Human Research at ICT.</p>

                  <div class="row">
                  <div class="col"></div>
                <div class="col-8">
                <img src="assets/img/brad.png" width="100%" height="300" alt="Four images (two in the upper row, two in the lower) displaying the experimental room for the Virtual Reality study."/>
                  </div>
                  <div class="col"></div>
                </div>

                  <p class="text-justify">
                    During my time as visiting researcher at USC Institute for Creative Technologies Sept-Dec 2016, I have worked on the implementation of the Conflict-Resolution Agent (CRA) as developed by <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.896.7570&rep=rep1&type=pdf"> Gratch et al. (2015)</a>, in immersive virtual reality (VR). In a within-subjects design, we explored the sense of presence and the usability of the systems are measured by means of questionnaires. The results show that there was a difference in the sense of presence, expressed in terms of involvement, awareness of the outside world and the degree to which all of participants’ senses were engaged. There was no difference in system usability, it is reasoned that this is resulting from the fact that both systems have clear advantages as well as implications. My personal goal as a visiting researcher was to gain new skills, such as hands-on with the Unity game engine, and 3D modeling of environments.
                   </p>

                   <div class="row">
                   <div class="col"></div>
                 <div class="col-8">
                 <img src="assets/img/classroomict.png" width="100%" height="200" alt="A physical room (left) and its 3D model (right) displayed side by side."/>
                   </div>
                   <div class="col"></div>
                 </div>


                   <h4 class="mb-5  mt-5">Spencer Project </h4>
                   <div class="row">
                   <div class="col"></div>
                 <div class="col-3">
                 <img src="assets/img/spencer.png" width="100%" height="300" alt="A virtual representation of a group of 3 people talking with the virtual 3D model of a Spencer robot."/>
             </div>
             <div class="col"></div>
           </div>

                   <p class="text-justify">
                     During my master, I was active as a student assistant in the European Spencer project. In research led by dr. Michiel Joosse, we conducted a survey on people’s perceptions of appropriate behavior of a guide robot at an airport. With this study, we have made a first step to investigate people’s beliefs of robot responses in conflict situations in public spaces, in particular airports.
                   </p>
                   <p class="text-justify">
                     Results suggest that in a variety of conflict situations, people’s belief of the robot’s action was not influenced, which could be interpreted as a permit to deploy robots with a single type of response to an action without taking the context into account. As a first exploration, we captured rich answers by collecting free-text responses, which limited the sample size, partly due to the resource-intense quantification of the qualitative data. This work has been published as a workshop paper, which can be found <a href="https://www.researchgate.net/publication/318135496_Groups_in_Conflict_at_the_Airport_How_People_Think_a_Robot_Should_Act" > at this link</a>.
                   </p>

                   <h4 class="mb-5  mt-5">BrainDiver: Design and Implementation of a Brain-Computer Interface (BCI) Game </h4>
                     <div class="row">
                     <div class="col"></div>
                   <div class="col-6">
                   <img src="assets/img/brain_diver.jpg" width="100%" height="200" alt="A participant playing the BrainDriver Brain-Computer Interface game."/>
               </div>
               <div class="col"></div>
             </div>


                   <p class="text-justify">
                     Together with two fellow students, I built a BCI-controlled 2D treasure hunt game. The game character is controlled using Steady-state visual evoked potentials (SSVEPs). Players steer the character in four directions (up, down, left, right) by focusing on a light, placed at the side of a computer screen in the respective position. The lights are flashing with different frequencies, which can be read out from the occipital areas of the brain (channel Oz).
                   </p>
                   <div class="row">
                   <div class="col"></div>
                 <div class="col-6">
                 <img src="assets/img/game_design.png" width="100%" height="200" alt="Schematic representation of the Game Design, displaying the different game elements with descriptive text explaining what the elements are for in the game."/>
                   </div>
                   <div class="col"></div>
                 </div>

                   <p class="text-justify" >
                     The game design of our Brain-Computer Interface (BCI) game, implemented for a master course.
                     We successfully conducted a small study (N=4) and evaluated the game compared to the same game with key controls. The results show that people had a more positive game experience with the SSVEP-controlled version as compared to the traditional key-control version. We encountered common problems with SSVEP, such as relatively low feeling of control and higher tiredness after playing the game compared to traditional key controls. However, in comparison to traditional key-control, the SSVEP-controlled game was generally rated more positive and had higher scores for immersion, flow and challenge.
                   </p>

                    <h4 class="mb-5  mt-5">Proxemics in Human-Robot Interaction in Virtual Reality </h4>
                    <div class="container">
                      <div class="row">
                      <div class="col-sm-1"></div>
                    <div class="col-sm-5">
                    <img src="assets/img/physical.jpg" width="100%" height="200px" alt="The physical experiment room." />
                  </div>
                   <div class="col-sm-5 mb-5">
                    <img src="assets/img/virtual_env.png" width="100%" height="200px" alt="The 3D virtual representation of the expeirment room." />
                  </div>
                </div>
                </div>
                    <p class="text-justify mt-5">
                      Together with one fellow student, I investigated proxemics, or more specifically, personal space between people and a robot in virtual reality (VR). We studied whether it is possible to measure personal distance in human-robot interaction in virtual reality. Two iterations of user studies were conducted to test our newly developed system. The system consists of a virtual representation of an experiment room in which a user is able to move around and interact with a virtual Giraff robot. In the first iteration, the influence of likeability and self-embodiment on personal space are studied.
                    </p>
                    <p class="text-justify">
                      In a first experiment, using a 2 (self-embodiment vs. no self-embodiment) by 3 (human face vs. smiley face vs. no face) within-subjects design, we explored the effects of self-embodiment and the robot’s face on people’s behavior in an approach task (participant approaches robot) and a stop task (robot approaches participant) in the virtual environment. The virtual environment resembled the appearance of the real room. We found that self-embodiment as implemented in this study, was not sufficient to improve people’s spatial perception in the VR environment. The arms were static and hardly noticed by participants, and in future work it would be interesting to implement tracking of the arms and legs, to provide more sophisticated self-embodiment. Results show that people preferred the smiley face of the robot, and we used this face for the robot in our second study.
                     </p>
                     <p class="text-justify">
                       In a second user study, using a between subjects (physical room vs. virtual room) we explored personal space kept between participants and the robot. Participants were asked to stand on a marked starting point on the floor.. A Giraff robot stood in front of the participants. In the virtual condition, the robot was a virtual representation of the Giraff robot and in the physical condition, a real version of the Giraff robot was used. The robot had patches attached to its front and its back. It were white patches, one of them had three numbers and the other had three names written on it. Participants were instructed to walk toward the robot, first to its back and then to its front, and to memorize the combination of the top number on the back patch and the top name on the front patch. They repeated this for a total of three combinations of a number and a name. Thus, in total, the participants walked towards the Giraff robot three times and memorized three combinations.
                     </p>

                     <div class="container">
                       <div class="row">
                       <div class="col-sm-1"></div>
                     <div class="col-sm-5">
                     <img src="assets/img/demo_virtual.png" width="100%" height="200" alt="The virtual experiment room from first-person perspective without showing avatar hands." />
                   </div>
                    <div class="col-sm-5 mb-5">
                     <img src="assets/img/embodiment.png" width="100%" height="200" alt="The virtual experiment room from first-person perspective showing avatar hands" />
                   </div>
                 </div>
                 </div>
                <p class="text-justify">
                  We looked at the walking patterns of the participants. The clustered area (x = 1.25, z = -0.75) is the starting point for the participants, the vertical line represents the threshold for determining a segment. The black box marks the location of the robot. The participants walked in a very similar pattern where the distance is roughly the same among the participants. We found that participants in the virtual condition tend to come rather close to the robot. This may partly be due to the location of the Oculus Rift in regard to the participant’s head. As the Oculus Rift is tracked slightly in front of the participant.
                </p> <p class="text-justify">

                  Our work suggests that it seems promising to measure personal space in virtual reality, as distances tend to be similar in virtual reality compared to distances in reality. However, it is important to acknowledge the potential limitations of a virtual approach using the Oculus Rift. As motion tracking takes place in front of the head, the objects in the environment seem to move even though they are stationary, creating a distorted experience of distance and making it more difficult to estimate distances. Moreover, the chosen task in this experiment may not generalize well to real-world human-robot interaction, because a) the robot was stationary and did not move, and b) the task required people to come close to the robot by design to read from its front and back.
                </p>


                <h4 class="mb-5  mt-5">Combining Cryptography and Cognitive Psychology - Bachelor Thesis </h4>

                  <img class="img-fluid img-logo mx-auto d-block" src="assets/img/sisl.png" alt="A schematic representation of the four-button serial interception sequence learning (SISL) task." />

                <p class="text-justify mt-5">
                  For my bachelor thesis for Psychology at the University of Twente, I came up with the idea to combine cryptography and cognitive psychology, in an attempt to improve password user authentication. The serial interception sequence learning (SISL) task was used in a user authentication system based on implicit motor sequence learning (<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.431.4440&rep=rep1&type=pdf">Bojinov, Sanchez, Reber, Boneh & Lincoln, 2014</a>). This study aimed to investigate the influence of training length on performance of the SISL task. The implementation was done in Matlab. Participants were distributed across three conditions: a training phase of 480 trials, 960 trials and 1,440 trials. Experiment 1 comprised two phases: a training phase and a test phase. In the former, participants substantially trained on a fixed sequence, in the latter, sequence knowledge was tested, using both the trained sequence and new, random sequences.
                </p>
                <p class="text-justify mt-5">
                Results showed that a short training is sufficient to create trained sequence advantage and that sequence knowledge remains over time. Evidence was found for the presence of memory consolidation. To enhance security, random bogus sequences and honey passwords were added to the SAS. It was showed that with the SAS it is difficult to dishonestly authenticate using password guessing attacks, at least more difficult than in most cases of traditional password authentication. However, future research should explore the robustness of the SISL-based authentication system. </p>

                [1] Bojinov, H., Sanchez, D., Reber, P., Boneh, D., & Lincoln, P. (2012). Neuroscience meets cryptography: designing crypto primitives secure against rubber hose attacks. In <i>21st {USENIX} Security Symposium ({USENIX} Security 12)</i> (pp. 129-141).

                </div>
                <div class="col"></div>
              </div>
            </section>
            <hr class="m-0" />
            <!-- Awards-->
            <section class="resume-section" id="contact">
                <div class="resume-section-content">
                  <div class="row">
                  <div class="col"></div>
                  <div class="col-4">
                    <h2 class="mb-5">Contact</h2>

                    <p> Sanne van Waveren <br>
                        PhD student Human-Robot Interaction <br>
                        KTH Royal Institute of Technology <br>
                        <i> Robotics, Perception and Learning (RPL) </i> <br>
                        Lindstedtsvägen 24, 4th floor <br>
                        SE-100 44 Stockholm, Sweden </p>

                        <p><a href="mailto:sannevw@kth.se"> sannevw@kth.se </a></p>
                        <p><a href="http://www.kth.se/profile/sannevw"> kth.se/profile/sanne </a></p>


                        <div class="social-icons">
                            <a class="social-icon" href="https://gist.github.com/Sannevw"><i class="fab fa-github"></i></a>
                            <a class="social-icon" href="https://twitter.com/sannevanwaveren?s=09"><i class="fab fa-twitter"></i></a>
                            <a class="social-icon" href="https://linkedin.com/in/svwaveren"><i class="fab fa-linkedin"></i></a>
                        </div>

                      </div>
                      <div class="col"></div>
                    </div>
                </div>
            </section>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
